{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "    lambda x,y: torch.abs(x-y),\n",
    "    lambda x,y: x * torch.exp(-y),\n",
    "    lambda x,y: torch.max(x,y),\n",
    "    lambda x,y: x - y * 2,\n",
    "    lambda x,y: torch.log(1 + x) + 2*y\n",
    "}\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotdict({\n",
    "    \"d_model\": 1024,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.0005,\n",
    "    \"wd\": 0.001,\n",
    "    \"n_layers\": 1,\n",
    "    \"max_range\": 100\n",
    "})\n",
    "X = torch.cartesian_prod(torch.arange(config.max_range), torch.arange(config.max_range))\n",
    "X_extrap = torch.cartesian_prod(torch.arange(config.max_range, 2*config.max_range), torch.arange(config.max_range, 2*config.max_range))\n",
    "y = torch.hstack([f(x,y) for x,y in X for f in functions]).view(-1, len(functions))\n",
    "y_extrap = torch.hstack([f(x,y) for x,y in X_extrap for f in functions]).view(-1, len(functions))\n",
    "scaler = MinMaxScaler((-1, 1))\n",
    "y = torch.tensor(scaler.fit_transform(y)).float()\n",
    "y_extrap = torch.tensor(scaler.transform(y_extrap)).float()\n",
    "validation_mask = torch.rand(len(X)) < 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(2, config.d_model)\n",
    "        self.lstm = nn.LSTM(config.d_model, config.d_model, config.n_layers, batch_first=True)\n",
    "        self.readout = nn.Sequential(nn.Linear(config.d_model * 2, len(functions)), nn.Tanh())\n",
    "\n",
    "        \n",
    "    def forward(self, operands):\n",
    "        output_seq = self.get_embeddings(operands.max()+1)\n",
    "        embedded_operands = output_seq.squeeze(0)[operands]\n",
    "        return self.readout(embedded_operands.flatten(1))\n",
    "    \n",
    "    def get_embeddings(self, max_range):\n",
    "        one_embed = self.embedding(torch.Tensor([1]).long())\n",
    "        inputs = one_embed.unsqueeze(1).expand(-1, max_range, -1)\n",
    "        output_seq, _ = self.lstm(inputs)\n",
    "        return output_seq\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding for integer 1\n",
    "- Increment to obtain new integers\n",
    "- Do addition or some other function on the integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 0.251, 14.167, 80.114:  35%|███▌      | 70/200 [00:47<01:29,  1.46it/s] "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "model = Model(config)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.wd)\n",
    "pbar = tqdm.trange(config.epochs)\n",
    "X_train = X[~validation_mask]\n",
    "X_val = X[validation_mask]\n",
    "y_train = y[~validation_mask]\n",
    "y_val = y[validation_mask]\n",
    "\n",
    "\n",
    "def unscale(x):\n",
    "    return torch.tensor(scaler.inverse_transform(x.detach().numpy()))\n",
    "\n",
    "\n",
    "y_unscaled = unscale(y_val)\n",
    "y_extra_unscaled = unscale(y_extrap)\n",
    "\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred_train = model(X_train)\n",
    "    train_loss = criterion(pred_train, y_train).sqrt()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    val_loss = criterion(unscale(model(X_val)), y_unscaled).sqrt()\n",
    "    loss_extrap = criterion(unscale(model(X_extrap)), y_extra_unscaled).sqrt()\n",
    "    pbar.set_description(\n",
    "        f\"Epoch {epoch}: {train_loss.item():.3f}, {val_loss.item():.3f}, {loss_extrap.item():.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(unscale(model(X_extrap)).detach().numpy(), y_extra_unscaled.detach().numpy(), \".\")\n",
    "plt.show()\n",
    "plt.hist(unscale(model(X_extrap)).detach().numpy(), bins=100, alpha=0.5, label=\"pred\")\n",
    "plt.hist(y_extra_unscaled.detach().numpy(), bins=100, alpha=0.5, label=\"true\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.get_embeddings(config.max_range).squeeze(0).detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
